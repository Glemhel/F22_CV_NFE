{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "JqTEm5A6_zlk",
        "3F75QqEw_9VU"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Frechet Inception Distance (FID) Calculation\n",
        "Reference: https://www.kaggle.com/code/ibtesama/gan-in-pytorch-with-fid#Fretchet-Inception-Distance"
      ],
      "metadata": {
        "id": "93eRXuCJc5Ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Function\n",
        "import torchvision.transforms as transforms\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import math\n",
        "import copy\n",
        "from scipy import linalg"
      ],
      "metadata": {
        "id": "ReCx5aBnPwzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/drive/My\\ Drive/CV2022"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_0lREE-QhYZ",
        "outputId": "157f26f7-69ce-4b7f-f891-420ac48d0a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/CV2022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Samples Generation"
      ],
      "metadata": {
        "id": "JqTEm5A6_zlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EqualLR:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "\n",
        "    def compute_weight(self, module):\n",
        "        weight = getattr(module, self.name + '_orig')\n",
        "        fan_in = weight.data.size(1) * weight.data[0][0].numel()\n",
        "\n",
        "        return weight * math.sqrt(2 / fan_in)\n",
        "\n",
        "    @staticmethod\n",
        "    def apply(module, name):\n",
        "        fn = EqualLR(name)\n",
        "\n",
        "        weight = getattr(module, name)\n",
        "        del module._parameters[name]\n",
        "        module.register_parameter(name + '_orig', nn.Parameter(weight.data))\n",
        "        module.register_forward_pre_hook(fn)\n",
        "\n",
        "        return fn\n",
        "\n",
        "    def __call__(self, module, input):\n",
        "        weight = self.compute_weight(module)\n",
        "        setattr(module, self.name, weight)\n",
        "\n",
        "\n",
        "def equal_lr(module, name='weight'):\n",
        "    EqualLR.apply(module, name)\n",
        "\n",
        "    return module"
      ],
      "metadata": {
        "id": "rG9jS-VvaSOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EqualLinear(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        linear = nn.Linear(in_dim, out_dim)\n",
        "        linear.weight.data.normal_()\n",
        "        linear.bias.data.zero_()\n",
        "\n",
        "        self.linear = equal_lr(linear)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)"
      ],
      "metadata": {
        "id": "KSMaXsrsaQ53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EqualConv2d(nn.Module):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        conv = nn.Conv2d(*args, **kwargs)\n",
        "        conv.weight.data.normal_()\n",
        "        conv.bias.data.zero_()\n",
        "        self.conv = equal_lr(conv)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.conv(input)"
      ],
      "metadata": {
        "id": "H90OUq7QaRdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConstantInput(nn.Module):\n",
        "    def __init__(self, channel, size=4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input = nn.Parameter(torch.randn(1, channel, size, size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch = x.shape[0]\n",
        "        out = self.input.repeat(batch, 1, 1, 1)\n",
        "        return out"
      ],
      "metadata": {
        "id": "InEq2RH2amvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NoiseInjection(nn.Module):\n",
        "    def __init__(self, channel):\n",
        "        super().__init__()\n",
        "\n",
        "        self.weight = nn.Parameter(torch.zeros(1, channel, 1, 1))\n",
        "\n",
        "    def forward(self, image, noise):\n",
        "        return image + self.weight * noise"
      ],
      "metadata": {
        "id": "QgG6rFIVamo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaptiveInstanceNorm(nn.Module):\n",
        "    def __init__(self, in_channel, style_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.norm = nn.InstanceNorm2d(in_channel)\n",
        "        self.style = nn.Linear(style_dim, in_channel * 2)\n",
        "        self.style.bias.data[:in_channel] = 1\n",
        "        self.style.bias.data[in_channel:] = 0\n",
        "\n",
        "    def forward(self, input, style):\n",
        "        style = self.style(style).unsqueeze(2).unsqueeze(3)\n",
        "        gamma, beta = style.chunk(2, 1)\n",
        "\n",
        "        out = self.norm(input)\n",
        "        out = gamma * out + beta\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "aMrrx-tnameR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BlurFunctionBackward(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, grad_output, kernel, kernel_flip):\n",
        "        ctx.save_for_backward(kernel, kernel_flip)\n",
        "\n",
        "        grad_input = F.conv2d(\n",
        "            grad_output, kernel_flip, padding=1, groups=grad_output.shape[1]\n",
        "        )\n",
        "\n",
        "        return grad_input\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, gradgrad_output):\n",
        "        kernel, kernel_flip = ctx.saved_tensors\n",
        "\n",
        "        grad_input = F.conv2d(\n",
        "            gradgrad_output, kernel, padding=1, groups=gradgrad_output.shape[1]\n",
        "        )\n",
        "\n",
        "        return grad_input, None, None\n",
        "\n",
        "\n",
        "class BlurFunction(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, kernel, kernel_flip):\n",
        "        ctx.save_for_backward(kernel, kernel_flip)\n",
        "\n",
        "        output = F.conv2d(input, kernel, padding=1, groups=input.shape[1])\n",
        "\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        kernel, kernel_flip = ctx.saved_tensors\n",
        "\n",
        "        grad_input = BlurFunctionBackward.apply(grad_output, kernel, kernel_flip)\n",
        "\n",
        "        return grad_input, None, None\n",
        "\n",
        "\n",
        "blur = BlurFunction.apply\n",
        "\n",
        "\n",
        "class Blur(nn.Module):\n",
        "    def __init__(self, channel):\n",
        "        super().__init__()\n",
        "\n",
        "        weight = torch.tensor([[1, 2, 1], [2, 4, 2], [1, 2, 1]], dtype=torch.float32)\n",
        "        weight = weight.view(1, 1, 3, 3)\n",
        "        weight = weight / weight.sum()\n",
        "        weight_flip = torch.flip(weight, [2, 3])\n",
        "\n",
        "        self.register_buffer('weight', weight.repeat(channel, 1, 1, 1))\n",
        "        self.register_buffer('weight_flip', weight_flip.repeat(channel, 1, 1, 1))\n",
        "\n",
        "    def forward(self, input):\n",
        "        return blur(input, self.weight, self.weight_flip)"
      ],
      "metadata": {
        "id": "z9O_rfpaas91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StyledConvBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channel,\n",
        "        out_channel,\n",
        "        kernel_size=3,\n",
        "        padding=1,\n",
        "        style_dim=512,\n",
        "        initial=False,\n",
        "        upsample=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        if initial:\n",
        "            self.conv1 = ConstantInput(in_channel)\n",
        "\n",
        "        else:\n",
        "            if upsample:\n",
        "                self.conv1 = nn.Sequential(\n",
        "                    nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "                    EqualConv2d(\n",
        "                        in_channel, out_channel, kernel_size, padding=padding\n",
        "                    ),\n",
        "                    Blur(out_channel),\n",
        "                )\n",
        "            else:\n",
        "                self.conv1 = EqualConv2d(\n",
        "                    in_channel, out_channel, kernel_size, padding=padding\n",
        "                )\n",
        "\n",
        "        self.noise1 = NoiseInjection(out_channel)\n",
        "        self.adain1 = AdaptiveInstanceNorm(out_channel, style_dim)\n",
        "        self.lrelu1 = nn.LeakyReLU(0.2)\n",
        "\n",
        "        self.conv2 = EqualConv2d(out_channel, out_channel, kernel_size, padding=padding)\n",
        "        self.noise2 = NoiseInjection(out_channel)\n",
        "        self.adain2 = AdaptiveInstanceNorm(out_channel, style_dim)\n",
        "        self.lrelu2 = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self, x, style, noise):\n",
        "        out = self.conv1(x)\n",
        "        out = self.noise1(out, noise)\n",
        "        out = self.lrelu1(out)\n",
        "        out = self.adain1(out, style)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.noise2(out, noise)\n",
        "        out = self.lrelu2(out)\n",
        "        out = self.adain2(out, style)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "XIbO8ZhIah05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim=512, n_linear=5):\n",
        "        super(Generator, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(n_linear):\n",
        "            layers.append(EqualLinear(z_dim, z_dim))\n",
        "            layers.append(nn.LeakyReLU(0.2))\n",
        "        self.style = nn.Sequential(*layers)\n",
        "        self.progression = nn.ModuleList(\n",
        "            [\n",
        "              StyledConvBlock(512, 512, 3, 1, initial=True),\n",
        "              StyledConvBlock(512, 512, 3, 1, upsample=True),\n",
        "              StyledConvBlock(512, 256, 3, 1, upsample=True),\n",
        "              StyledConvBlock(256, 128, 3, 1, upsample=True),\n",
        "              StyledConvBlock(128, 64, 3, 1, upsample=True),\n",
        "            ]\n",
        "        )\n",
        "        self.to_rgb = EqualConv2d(64, 3, 1)\n",
        "\n",
        "    def forward(self, x, noise=None, step=0):\n",
        "        batch = x.size(0)\n",
        "        if noise is None:\n",
        "            noise = []\n",
        "            for i in range(step + 1):\n",
        "                size = 4 * 2 ** i\n",
        "                noise.append(torch.randn(batch, 1, size, size, device=x[0].device))\n",
        "        x = x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + 1e-8)\n",
        "        styles = self.style(x)\n",
        "        out = noise[0]\n",
        "        for i, conv in enumerate(self.progression):\n",
        "            out = self.progression[i](out, styles, noise[i])\n",
        "        return self.to_rgb(out)"
      ],
      "metadata": {
        "id": "w8CHA_vzaDsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDOIOPfwKbpe"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n",
        "G = torch.load('Generator_v2_150.pth', map_location=device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_save_random_image(G, index):\n",
        "    img_size = 64\n",
        "    step = int(math.log(img_size, 2)) - 2\n",
        "    z = torch.randn((1, 512))\n",
        "    with torch.no_grad():\n",
        "        img = G(z, step=step)[0]\n",
        "    imgpath = f'generated_images/random_image_{index}.png'\n",
        "    imgdata = torch.clip(img, 0, 1).permute([1, 2, 0]).detach().cpu().numpy()\n",
        "    plt.imsave(imgpath, imgdata)\n",
        "    return imgpath, z"
      ],
      "metadata": {
        "id": "tQGTJLzUKpJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10000):\n",
        "    generate_and_save_random_image(G, i)"
      ],
      "metadata": {
        "id": "fOTmH8fcbU4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datasets Construction"
      ],
      "metadata": {
        "id": "3F75QqEw_9VU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        super().__init__()\n",
        "        self.files = glob.glob(root_dir+\"/*\")\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.files[idx]).convert(\"RGB\")\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        return image"
      ],
      "metadata": {
        "id": "kEGDUIgjtTlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataroot = \"animefacedataset/images\"\n",
        "\n",
        "img_size = 64\n",
        "batch_size = 1\n",
        "dataset = CustomDataset(root_dir=dataroot,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize((img_size, img_size)),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ]))\n",
        "# Create the dataloader\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "QrnxZ37EtV-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_dataroot = \"generated_images\"\n",
        "\n",
        "img_size = 64\n",
        "batch_size = 1\n",
        "gen_dataset = CustomDataset(root_dir=gen_dataroot,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize((img_size, img_size)),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ]))\n",
        "# Create the dataloader\n",
        "gen_dataloader = torch.utils.data.DataLoader(gen_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "KqGMNsaWbc9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_images_array = np.zeros((10000, 3, 64, 64))\n",
        "gen_images_array = np.zeros((10000, 3, 64, 64))\n",
        "\n",
        "iter_dataloader = iter(dataloader)\n",
        "iter_gen_dataloader = iter(gen_dataloader)\n",
        "\n",
        "for i in range(10000):\n",
        "    if i % 10 == 0:\n",
        "        print(i)\n",
        "    init_images_array[i] = next(iter_dataloader)[0].numpy()\n",
        "    gen_images_array[i] = next(iter_gen_dataloader)[0].numpy()"
      ],
      "metadata": {
        "id": "gswCTMD8mT0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_images_array.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8AV1u-0oJG1",
        "outputId": "43e5066a-60a1-43b8-bdc7-e2628e962593"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 3, 64, 64)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(init_images_array[9999].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pi5-hp1tsJrd",
        "outputId": "416a58df-383f-4c81-a1ab-814d69aedc85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 64, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# np.save('init_array', init_images_array)\n",
        "# np.save('gen_array', gen_images_array)"
      ],
      "metadata": {
        "id": "50-DHR0Pxcst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FID Calculation"
      ],
      "metadata": {
        "id": "Xho1pNJ5ABdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
        "    \"\"\"Numpy implementation of the Frechet Distance.\n",
        "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
        "    and X_2 ~ N(mu_2, C_2) is\n",
        "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
        "    \"\"\"\n",
        "\n",
        "    mu1 = np.atleast_1d(mu1)\n",
        "    mu2 = np.atleast_1d(mu2)\n",
        "\n",
        "    sigma1 = np.atleast_2d(sigma1)\n",
        "    sigma2 = np.atleast_2d(sigma2)\n",
        "\n",
        "    assert mu1.shape == mu2.shape, \\\n",
        "        'Training and test mean vectors have different lengths'\n",
        "    assert sigma1.shape == sigma2.shape, \\\n",
        "        'Training and test covariances have different dimensions'\n",
        "\n",
        "    diff = mu1 - mu2\n",
        "\n",
        "    \n",
        "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
        "    if not np.isfinite(covmean).all():\n",
        "        msg = ('fid calculation produces singular product; '\n",
        "               'adding %s to diagonal of cov estimates') % eps\n",
        "        print(msg)\n",
        "        offset = np.eye(sigma1.shape[0]) * eps\n",
        "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
        "\n",
        "    \n",
        "    if np.iscomplexobj(covmean):\n",
        "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
        "            m = np.max(np.abs(covmean.imag))\n",
        "            raise ValueError('Imaginary component {}'.format(m))\n",
        "        covmean = covmean.real\n",
        "\n",
        "    tr_covmean = np.trace(covmean)\n",
        "\n",
        "    return (diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean)"
      ],
      "metadata": {
        "id": "xnZnu54hM6-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_images_array = np.load('init_array.npy')\n",
        "gen_images_array = np.load('gen_array.npy')"
      ],
      "metadata": {
        "id": "o-uUX7zy_6sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_images_array.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dK9WHreYA-eK",
        "outputId": "23851500-3cfc-4493-c4e1-f7babd36c35b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 3, 64, 64)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fids = []\n",
        "colors = ['RED', 'GREEN', 'BLUE']\n",
        "sizes = [[[0, 32], [0, 32]], [[32, 64], [0, 32]], [[0, 32], [32, 64]], [[32, 64], [32, 64]]]\n",
        "\n",
        "for i in range(3):\n",
        "    print('channel: ', colors[i])\n",
        "    for j in range(4):\n",
        "        current_quarter = sizes[j]\n",
        "        x1, x2 = current_quarter[0]\n",
        "        y1, y2 = current_quarter[1]\n",
        "        print('quarter: ', current_quarter)\n",
        "\n",
        "        init_part = copy.deepcopy(init_images_array[:10000, i:i + 1, x1:x2, y1:y2])\n",
        "        gen_part = copy.deepcopy(gen_images_array[:10000, i:i + 1, x1:x2, y1:y2])\n",
        "        # print('initial shape: ', init_part.shape)\n",
        "\n",
        "        init_2D = init_part.reshape(init_part.shape[0], -1)\n",
        "        gen_2D = gen_part.reshape(gen_part.shape[0], -1)\n",
        "        # print('2D shape: ', init_2D.shape)\n",
        "\n",
        "        init_mu = np.mean(init_2D, axis=0)\n",
        "        init_sigma = np.cov(init_2D, rowvar=False)\n",
        "\n",
        "        gen_mu = np.mean(gen_2D, axis=0)\n",
        "        gen_sigma = np.cov(gen_2D, rowvar=False)\n",
        "\n",
        "        current_fid = calculate_frechet_distance(init_mu, init_sigma, gen_mu, gen_sigma)\n",
        "        fids.append(current_fid)\n",
        "        print('FID: ', current_fid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CqfEImAAdzo",
        "outputId": "01380a8f-a3ca-4dc8-fcdd-7214df830fe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "channel:  RED\n",
            "quarter:  [[0, 32], [0, 32]]\n",
            "FID:  209.2895935186839\n",
            "quarter:  [[32, 64], [0, 32]]\n",
            "FID:  203.3839375458764\n",
            "quarter:  [[0, 32], [32, 64]]\n",
            "FID:  291.6656503531822\n",
            "quarter:  [[32, 64], [32, 64]]\n",
            "FID:  210.04824968332048\n",
            "channel:  GREEN\n",
            "quarter:  [[0, 32], [0, 32]]\n",
            "FID:  344.65171807566\n",
            "quarter:  [[32, 64], [0, 32]]\n",
            "FID:  269.7404155800184\n",
            "quarter:  [[0, 32], [32, 64]]\n",
            "FID:  432.3298171981106\n",
            "quarter:  [[32, 64], [32, 64]]\n",
            "FID:  298.1838433448992\n",
            "channel:  BLUE\n",
            "quarter:  [[0, 32], [0, 32]]\n",
            "FID:  420.66032142787185\n",
            "quarter:  [[32, 64], [0, 32]]\n",
            "FID:  295.78259560661604\n",
            "quarter:  [[0, 32], [32, 64]]\n",
            "FID:  515.0935615107355\n",
            "quarter:  [[32, 64], [32, 64]]\n",
            "FID:  334.66293931552804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "overall_mean = 0\n",
        "for i in range(0, 12, 4):\n",
        "    print('channel: ', colors[i % 3])\n",
        "    current_mean = np.mean(fids[i:i + 4])\n",
        "    overall_mean += current_mean\n",
        "    print('mean: ', current_mean)\n",
        "\n",
        "overall_mean /= 3\n",
        "print('OVERALL FID: ', overall_mean)"
      ],
      "metadata": {
        "id": "NwSkkix37Vb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db0c3537-40a1-491e-fb32-b7a4369e7fda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "channel:  RED\n",
            "mean:  228.59685777526573\n",
            "channel:  GREEN\n",
            "mean:  336.2264485496721\n",
            "channel:  BLUE\n",
            "mean:  391.5498544651879\n",
            "OVERALL FID:  318.7910535967086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8v5GLJs3ZDT8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}